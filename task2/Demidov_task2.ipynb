{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>DocNumber</th>\n",
       "      <th>MetadataSubject</th>\n",
       "      <th>MetadataTo</th>\n",
       "      <th>MetadataFrom</th>\n",
       "      <th>SenderPersonId</th>\n",
       "      <th>MetadataDateSent</th>\n",
       "      <th>MetadataDateReleased</th>\n",
       "      <th>MetadataPdfLink</th>\n",
       "      <th>MetadataCaseNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>ExtractedTo</th>\n",
       "      <th>ExtractedFrom</th>\n",
       "      <th>ExtractedCc</th>\n",
       "      <th>ExtractedDateSent</th>\n",
       "      <th>ExtractedCaseNumber</th>\n",
       "      <th>ExtractedDocNumber</th>\n",
       "      <th>ExtractedDateReleased</th>\n",
       "      <th>ExtractedReleaseInPartOrFull</th>\n",
       "      <th>ExtractedBodyText</th>\n",
       "      <th>RawText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>C05739545</td>\n",
       "      <td>WOW</td>\n",
       "      <td>H</td>\n",
       "      <td>Sullivan, Jacob J</td>\n",
       "      <td>87</td>\n",
       "      <td>2012-09-12T04:00:00+00:00</td>\n",
       "      <td>2015-05-22T04:00:00+00:00</td>\n",
       "      <td>DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739545...</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sullivan, Jacob J &lt;Sullivan11@state.gov&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wednesday, September 12, 2012 10:16 AM</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>C05739545</td>\n",
       "      <td>05/13/2015</td>\n",
       "      <td>RELEASE IN FULL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNCLASSIFIED\\nU.S. Department of State\\nCase N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C05739546</td>\n",
       "      <td>H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-03-03T05:00:00+00:00</td>\n",
       "      <td>2015-05-22T04:00:00+00:00</td>\n",
       "      <td>DOCUMENTS/HRC_Email_1_296/HRCH1/DOC_0C05739546...</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>C05739546</td>\n",
       "      <td>05/13/2015</td>\n",
       "      <td>RELEASE IN PART</td>\n",
       "      <td>B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest...</td>\n",
       "      <td>UNCLASSIFIED\\nU.S. Department of State\\nCase N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C05739547</td>\n",
       "      <td>CHRIS STEVENS</td>\n",
       "      <td>;H</td>\n",
       "      <td>Mills, Cheryl D</td>\n",
       "      <td>32</td>\n",
       "      <td>2012-09-12T04:00:00+00:00</td>\n",
       "      <td>2015-05-22T04:00:00+00:00</td>\n",
       "      <td>DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739547...</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>...</td>\n",
       "      <td>B6</td>\n",
       "      <td>Mills, Cheryl D &lt;MillsCD@state.gov&gt;</td>\n",
       "      <td>Abedin, Huma</td>\n",
       "      <td>Wednesday, September 12, 2012 11:52 AM</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>C05739547</td>\n",
       "      <td>05/14/2015</td>\n",
       "      <td>RELEASE IN PART</td>\n",
       "      <td>Thx</td>\n",
       "      <td>UNCLASSIFIED\\nU.S. Department of State\\nCase N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>C05739550</td>\n",
       "      <td>CAIRO CONDEMNATION - FINAL</td>\n",
       "      <td>H</td>\n",
       "      <td>Mills, Cheryl D</td>\n",
       "      <td>32</td>\n",
       "      <td>2012-09-12T04:00:00+00:00</td>\n",
       "      <td>2015-05-22T04:00:00+00:00</td>\n",
       "      <td>DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739550...</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mills, Cheryl D &lt;MillsCD@state.gov&gt;</td>\n",
       "      <td>Mitchell, Andrew B</td>\n",
       "      <td>Wednesday, September 12,2012 12:44 PM</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>C05739550</td>\n",
       "      <td>05/13/2015</td>\n",
       "      <td>RELEASE IN PART</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNCLASSIFIED\\nU.S. Department of State\\nCase N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C05739554</td>\n",
       "      <td>H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...</td>\n",
       "      <td>Abedin, Huma</td>\n",
       "      <td>H</td>\n",
       "      <td>80</td>\n",
       "      <td>2011-03-11T05:00:00+00:00</td>\n",
       "      <td>2015-05-22T04:00:00+00:00</td>\n",
       "      <td>DOCUMENTS/HRC_Email_1_296/HRCH1/DOC_0C05739554...</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>C05739554</td>\n",
       "      <td>05/13/2015</td>\n",
       "      <td>RELEASE IN PART</td>\n",
       "      <td>H &lt;hrod17@clintonemail.com&gt;\\nFriday, March 11,...</td>\n",
       "      <td>B6\\nUNCLASSIFIED\\nU.S. Department of State\\nCa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  DocNumber                                    MetadataSubject  \\\n",
       "0   1  C05739545                                                WOW   \n",
       "1   2  C05739546  H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...   \n",
       "2   3  C05739547                                      CHRIS STEVENS   \n",
       "3   4  C05739550                         CAIRO CONDEMNATION - FINAL   \n",
       "4   5  C05739554  H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...   \n",
       "\n",
       "     MetadataTo       MetadataFrom  SenderPersonId           MetadataDateSent  \\\n",
       "0             H  Sullivan, Jacob J              87  2012-09-12T04:00:00+00:00   \n",
       "1             H                NaN             NaN  2011-03-03T05:00:00+00:00   \n",
       "2            ;H    Mills, Cheryl D              32  2012-09-12T04:00:00+00:00   \n",
       "3             H    Mills, Cheryl D              32  2012-09-12T04:00:00+00:00   \n",
       "4  Abedin, Huma                  H              80  2011-03-11T05:00:00+00:00   \n",
       "\n",
       "        MetadataDateReleased  \\\n",
       "0  2015-05-22T04:00:00+00:00   \n",
       "1  2015-05-22T04:00:00+00:00   \n",
       "2  2015-05-22T04:00:00+00:00   \n",
       "3  2015-05-22T04:00:00+00:00   \n",
       "4  2015-05-22T04:00:00+00:00   \n",
       "\n",
       "                                     MetadataPdfLink MetadataCaseNumber  \\\n",
       "0  DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739545...       F-2015-04841   \n",
       "1  DOCUMENTS/HRC_Email_1_296/HRCH1/DOC_0C05739546...       F-2015-04841   \n",
       "2  DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739547...       F-2015-04841   \n",
       "3  DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739550...       F-2015-04841   \n",
       "4  DOCUMENTS/HRC_Email_1_296/HRCH1/DOC_0C05739554...       F-2015-04841   \n",
       "\n",
       "                         ...                         ExtractedTo  \\\n",
       "0                        ...                                 NaN   \n",
       "1                        ...                                 NaN   \n",
       "2                        ...                                  B6   \n",
       "3                        ...                                 NaN   \n",
       "4                        ...                                 NaN   \n",
       "\n",
       "                              ExtractedFrom         ExtractedCc  \\\n",
       "0  Sullivan, Jacob J <Sullivan11@state.gov>                 NaN   \n",
       "1                                       NaN                 NaN   \n",
       "2       Mills, Cheryl D <MillsCD@state.gov>        Abedin, Huma   \n",
       "3       Mills, Cheryl D <MillsCD@state.gov>  Mitchell, Andrew B   \n",
       "4                                       NaN                 NaN   \n",
       "\n",
       "                        ExtractedDateSent ExtractedCaseNumber  \\\n",
       "0  Wednesday, September 12, 2012 10:16 AM        F-2015-04841   \n",
       "1                                     NaN        F-2015-04841   \n",
       "2  Wednesday, September 12, 2012 11:52 AM        F-2015-04841   \n",
       "3   Wednesday, September 12,2012 12:44 PM        F-2015-04841   \n",
       "4                                     NaN        F-2015-04841   \n",
       "\n",
       "  ExtractedDocNumber ExtractedDateReleased ExtractedReleaseInPartOrFull  \\\n",
       "0          C05739545            05/13/2015              RELEASE IN FULL   \n",
       "1          C05739546            05/13/2015              RELEASE IN PART   \n",
       "2          C05739547            05/14/2015              RELEASE IN PART   \n",
       "3          C05739550            05/13/2015              RELEASE IN PART   \n",
       "4          C05739554            05/13/2015              RELEASE IN PART   \n",
       "\n",
       "                                   ExtractedBodyText  \\\n",
       "0                                                NaN   \n",
       "1  B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest...   \n",
       "2                                                Thx   \n",
       "3                                                NaN   \n",
       "4  H <hrod17@clintonemail.com>\\nFriday, March 11,...   \n",
       "\n",
       "                                             RawText  \n",
       "0  UNCLASSIFIED\\nU.S. Department of State\\nCase N...  \n",
       "1  UNCLASSIFIED\\nU.S. Department of State\\nCase N...  \n",
       "2  UNCLASSIFIED\\nU.S. Department of State\\nCase N...  \n",
       "3  UNCLASSIFIED\\nU.S. Department of State\\nCase N...  \n",
       "4  B6\\nUNCLASSIFIED\\nU.S. Department of State\\nCa...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ds = pd.read_csv(\"dataset/Emails.csv\")\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "PUNCTUATION = \"!\\\"(),.:;?[]{}…“\"\n",
    "BAD_SYMBOLS = \"#$%&*+/<=>@\\^|~â©ñ¹_\" + string.digits\n",
    "\n",
    "\n",
    "def filtered(text):\n",
    "    for symbol in BAD_SYMBOLS:\n",
    "        if text.find(symbol) != -1 or len(text) < 2:\n",
    "            return ''\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def transform_raw_texts(raw_texts):\n",
    "    transformed_texts = []\n",
    "    \n",
    "    for line in raw_texts:\n",
    "        transformed_line = \"\"\n",
    "        \n",
    "        for character in PUNCTUATION:\n",
    "            line = line.replace(character, \" \")\n",
    "\n",
    "        for word in line.strip('\\n').split():\n",
    "            filtered_word = filtered(word)\n",
    "\n",
    "            if filtered_word != '':\n",
    "                transformed_line += filtered_word.lower() + \" \"\n",
    "                \n",
    "        transformed_texts.append(transformed_line)\n",
    "\n",
    "    return transformed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7945\n"
     ]
    }
   ],
   "source": [
    "texts = transform_raw_texts(ds[\"RawText\"].tolist())\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for text in texts:\n",
    "    tokens += text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('of', 'state'), 28129), (('department', 'of'), 27169), (('doc', 'no'), 26534), (('case', 'no'), 26527), (('state', 'case'), 26515), (('no', 'date'), 26510), (('unclassified', 'department'), 26509), (('no', 'doc'), 26508), (('of', 'the'), 14052), (('date', 'unclassified'), 12830), (('in', 'the'), 10284), (('release', 'in'), 7916), (('date', 'release'), 7501), (('original', 'message'), 7012), (('subject', 're'), 6728), (('to', 'the'), 6538), (('message', 'from'), 6115), (('pm', 'to'), 5613), (('on', 'the'), 5170), (('to', 'subject'), 4484)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from itertools import islice\n",
    "\n",
    "frequencies = collections.Counter(zip(tokens, islice(tokens, 1, None)))\n",
    "\n",
    "print(frequencies.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"''rn\", 'rince'),\n",
       " (\"''s\", 'pone'),\n",
       " (\"'--\", 'twatch'),\n",
       " (\"'alexandre\", \"carl'\"),\n",
       " (\"'amitabh\", \"desai'\"),\n",
       " (\"'baby\", \"steps'\"),\n",
       " (\"'badly\", \"trained'\"),\n",
       " (\"'baer\", 'danie'),\n",
       " (\"'barry\", \"keith'\"),\n",
       " (\"'beitler\", \"ady'\"),\n",
       " (\"'benderslw\", \"matias'\"),\n",
       " (\"'big\", \"bang'\"),\n",
       " (\"'brian'\", \"'brocking\"),\n",
       " (\"'brocking\", \"elisabeth'\"),\n",
       " (\"'buhl\", \"cindy'\"),\n",
       " (\"'chang\", \"benjamin'\"),\n",
       " (\"'cheer\", \"on'\"),\n",
       " (\"'civ\", \"cas'\"),\n",
       " (\"'confidence\", \"building'\"),\n",
       " (\"'damage\", \"limitation'\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "finder.nbest(bigram_measures.pmi, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def log_likelihood(a, b, c, d):\n",
    "    if a == 0 or b == 0 or c == 0 or d == 0:\n",
    "        return 0\n",
    "\n",
    "    return 2 * (a * log(a) + b * log(b) + c * log(c) + d * log(d) + (a + b + c + d) * log(a + b + c + d) -\n",
    "                (a + b) * log(a + b) - (a + c) * log(a + c) - (b + d) * log(b + d) - (c + d) * log(c + d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_word_to_dict(current_dict, word):\n",
    "    if word == '':\n",
    "        return\n",
    "\n",
    "    if current_dict.get(word) is None:\n",
    "        current_dict[word] = 1\n",
    "    else:\n",
    "        current_dict[word] += 1\n",
    "\n",
    "\n",
    "def add_text_to_dict(current_dict, text):\n",
    "    for token in text.split():\n",
    "        if current_dict.get(token) is None:\n",
    "            current_dict[token] = 1\n",
    "        else:\n",
    "            current_dict[token] += 1\n",
    "\n",
    "\n",
    "def reduce_lexicon_dimension(current_dict, stop_words_number):\n",
    "    words = sorted(current_dict.items(), key=lambda x: -x[1])\n",
    "\n",
    "    return {item[0]: item[1] for item in words[stop_words_number:]}, \\\n",
    "           {item[0]: item[1] for item in words[:stop_words_number]}\n",
    "\n",
    "\n",
    "def current_text_keywords(current_text_dict, normative_dict, total_number_of_words, stop_words_dict, ll_score_threshold):\n",
    "    for word in stop_words_dict:\n",
    "        current_text_dict.pop(word, None)\n",
    "\n",
    "    current_number_of_words = sum(current_text_dict.values())\n",
    "\n",
    "    current_scores_dict = {}\n",
    "    for word in current_text_dict:\n",
    "        ll_score = log_likelihood(current_text_dict[word],\n",
    "                                  normative_dict[word],\n",
    "                                  current_number_of_words - current_text_dict[word],\n",
    "                                  total_number_of_words - normative_dict[word])\n",
    "\n",
    "        if ll_score >= ll_score_threshold:\n",
    "            current_scores_dict[word] = ll_score\n",
    "\n",
    "    return [item[0] for item in sorted(current_scores_dict.items(), key=lambda x: -x[1])]\n",
    "\n",
    "\n",
    "def build_normative_dict(texts):\n",
    "    result_dict = {}\n",
    "    \n",
    "    for text in texts:\n",
    "        add_text_to_dict(result_dict, text)\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def get_keywords(texts, ll_score_threshold, normative_dict, stop_words_dict,\n",
    "                 total_number_of_words, lexicon_dimension):\n",
    "\n",
    "    keywords_dict = {}\n",
    "\n",
    "    for text in texts:\n",
    "        current_text_dict = {}\n",
    "        add_text_to_dict(current_text_dict, text)\n",
    "\n",
    "        current_keywords = current_text_keywords(current_text_dict, normative_dict, total_number_of_words,\n",
    "                                                 stop_words_dict, ll_score_threshold)\n",
    "\n",
    "        for word in current_keywords:\n",
    "            add_word_to_dict(keywords_dict, word)\n",
    "\n",
    "    return set([item[0] for item in sorted(keywords_dict.items(), key=lambda x: -x[1])[:lexicon_dimension]])\n",
    "\n",
    "\n",
    "def extract_features(texts, keywords_vector_dimension, stop_words_num, lexicon_dimension, ll_score_threshold):\n",
    "\n",
    "    normative_dict = build_normative_dict(texts)\n",
    "    normative_dict, stop_words_dict = reduce_lexicon_dimension(normative_dict, stop_words_num)\n",
    "    total_number_of_words = sum(normative_dict.values())\n",
    "    \n",
    "    keywords = get_keywords(texts, ll_score_threshold, normative_dict,\n",
    "                            stop_words_dict, total_number_of_words, lexicon_dimension)\n",
    "    \n",
    "    keywords_for_texts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        current_text_dict = {}\n",
    "        add_text_to_dict(current_text_dict, text)\n",
    "\n",
    "        raw_current_keywords = current_text_keywords(current_text_dict, normative_dict, total_number_of_words,\n",
    "                                                     stop_words_dict, ll_score_threshold)\n",
    "\n",
    "        current_keywords = []\n",
    "\n",
    "        for keyword in raw_current_keywords:\n",
    "            if keyword in keywords:\n",
    "                 current_keywords.append(keyword)\n",
    "                    \n",
    "        keywords_for_text = \"\"\n",
    "        \n",
    "        for keyword in current_keywords[:keywords_vector_dimension]:\n",
    "            keywords_for_text += keyword + \" \"\n",
    "\n",
    "        keywords_for_texts.append(keywords_for_text)\n",
    "\n",
    "    return keywords_for_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster.hierarchical import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_kmeans_clusters_map(texts, CLUSTERS_NUMBER):\n",
    "    vectorizer = CountVectorizer()\n",
    "    matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    print(\"Vectorizer matrix shape: \" + str(matrix.shape))\n",
    "\n",
    "    model = KMeans(n_clusters=CLUSTERS_NUMBER, random_state=1)\n",
    "    return model.fit_predict(matrix.toarray())\n",
    "\n",
    "def get_agglomerative_clusters_map(texts, CLUSTERS_NUMBER):\n",
    "    vectorizer = CountVectorizer()\n",
    "    matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "    print(\"Vectorizer matrix shape: \" + str(matrix.shape))\n",
    "\n",
    "    model = AgglomerativeClustering(n_clusters=CLUSTERS_NUMBER, affinity='euclidean', linkage='ward')\n",
    "    return model.fit_predict(matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clusters_as_texts(texts, clusters_map, CLUSTERS_NUMBER):\n",
    "    clusters = [\"\" for i in range(CLUSTERS_NUMBER)]\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        clusters[clusters_map[i]] += text + \" \"\n",
    "        \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KEYWORDS_VECTOR_DIMENSION = 30\n",
    "STOP_WORDS_NUM = 50\n",
    "LEXICON_DIMENSION = 10000\n",
    "LL_SCORE_THRESHOLD = 10.83\n",
    "CLUSTERS_NUMBER = 20\n",
    "\n",
    "keywords_for_texts = extract_features(texts, LEXICON_DIMENSION, STOP_WORDS_NUM, LEXICON_DIMENSION, LL_SCORE_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer matrix shape: (7945, 9367)\n"
     ]
    }
   ],
   "source": [
    "clusters_map = get_kmeans_clusters_map(keywords_for_texts, CLUSTERS_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = clusters_as_texts(texts, clusters_map, CLUSTERS_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "susman shaun cameron boxed worked compelled perfectly beforehand lou call clarification reg made convenient empey sid considering timing february helpful felt seeking couple tactic hour half works afternoon event late \n",
      "\n",
      "baer lona dan muscatine lissa valmoro speech draft ornament refinement re-working dec agree ornaments 'daniel works getting topple huma tree sun daniel cc careful closer copying plane abedin raise anyone \n",
      "\n",
      "lauren jiloty call mashabane aug list update huma abedin cc thu tomorrow august mailto morning sat anytime can com do him ok dep i'm menendez armitage wexler fmr ill me \n",
      "\n",
      "room hrc dr hardy office sharon secretary's signs floor appointment guests photographer staff franklin pascual tauscher contact affidavit participants makes administers remarks escort ambassador seasons greet departs merten photos proceeds \n",
      "\n",
      "children moyes child trafficking haiti unicef save jillian kids disaster thousands earthquake adoption concern families westportnow atlanta need desperate agencies quake php island muscatine parents lissa est hit january exploitation \n",
      "\n",
      "colombia colombian cindy uribe's alston paramilitary secretary uribe colombia's alvaro bogota buhl murders important brief pastoral memo positives extrajudicial cipcol defense impunity defenders prosecution posner prosecutors das cases false political \n",
      "\n",
      "deal dup power-sharing guardian ireland co northern sinn devolved assembly unionist policing fein brava bravo brown powers robinson belfast agreement allister unionists chapter parades gordon party negotiations justice parties tuv \n",
      "\n",
      "peace arab israel arabs palestinian holy land palestinians process american oslo israelis israeli middle israel-palestine israel's states east jewish their saudi mufaawadhat terrorism its between muslims diplomatic interests strife reasons \n",
      "\n",
      "secretary's office depart arrive route room meeting residence private en daily mini airport conference time hotel floor staff drive schedule outer bilateral briefing spray white pelletier preceding treaty photo jiloty \n",
      "\n",
      "heal fistula africa hospital goma congolese kay lyn kivu lusi congo warren mrs excellent volcano local based training north visit trained centre teaching hey patients actors maniema recognized boss through \n",
      "\n",
      "benghazi sensitive select redactions foia comm waiver produced dept information agreement libya house libyan source magariaf qaddafi huma western abedin militias individual el-keib jalil attack forces according belhaj intelligence ntc \n",
      "\n",
      "tories clegg brown labour cameron outright election lds murdoch tory lib seats attempt majority hague conservatives peter short referendum party win deal falling dems liberal coalition brown's poll guardian uk \n",
      "\n",
      "windrush ventures tony registered info limited email obl aclb lp broadway blair sep lynn messagelabs wales sullivan any london office viruses england sullivar forester aspen teddy rothschild best 'jake confidential \n",
      "\n",
      "guard contract embassy guards kabul agna dc sep tue force downloaded security wh pogo supervisors contractor contractors governmental wackenhut combat inherently armorgroup visa wait abedin huma august ng washington http \n",
      "\n",
      "idea thematically speeches trilogy water theme human speech health food development muscatine ctr qua elevation lissa sine underscore our prism lm major survival linking why non pr existence done three \n",
      "\n",
      "branch gore wjc north korea korean south asean carter asia clinton had lewinsky moynihan wjc's reno gore's kim hrc korea's cambodia seoul lee writes cvc pyongyang summit book japan malaysia \n",
      "\n",
      "treaty cameron cameron's letter europe tory eu lisbon guardian zapatero czech klaus understood referendum leaders attempt merkel opt-out sarkozy scupper incensed conservative ratified european criticised tories abandon party blowup vaclav \n",
      "\n",
      "benghazi sensitive redactions foia select comm waiver produced dept magariaf agreement libyan libya secretary's jiloty lauren depart information route arrive source qaddafi house residence militias el-keib room belhaj mashabane branch \n",
      "\n",
      "en-us gov recipient's diagnostic microsoft e-mail received undeliverable washdc message-id address jan followup x-ms-has-attach esmtp mapi mime-version x-originalarrivaltime x-ms-tnef-correlator headers content-language smtpsvc return-path redeliver thread-index content-type thread-topic accept-language content-transfer-encoding system \n",
      "\n",
      "jamal deuel place dcfs kids boys says home scoresheet marlowe wards ebeling when juvenile measure grandmother sad illinois contract renew improvements children got staff facility betsy residents naperville graystone corrine \n",
      "\n"
     ]
    }
   ],
   "source": [
    "keywords_for_clusters = extract_features(clusters, KEYWORDS_VECTOR_DIMENSION, \n",
    "                                         STOP_WORDS_NUM, LEXICON_DIMENSION, LL_SCORE_THRESHOLD)\n",
    "\n",
    "for cluster_keywords in keywords_for_clusters:\n",
    "    print(cluster_keywords + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_clusters(clusters_map, clusters_number):\n",
    "    number_of_documents = 0\n",
    "    clusters_consistency = [0 for i in range(clusters_number)]\n",
    "\n",
    "    for cluster in clusters_map:\n",
    "        clusters_consistency[int(cluster)] += 1\n",
    "        number_of_documents += 1\n",
    "\n",
    "    for i in range(clusters_number):\n",
    "        print(str(i) + ' ' + str(clusters_consistency[i]) + ' ' + str(clusters_consistency[i] / number_of_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4 0.0005034612964128383\n",
      "1 4 0.0005034612964128383\n",
      "2 206 0.02592825676526117\n",
      "3 2 0.0002517306482064191\n",
      "4 5 0.0006293266205160479\n",
      "5 6 0.0007551919446192574\n",
      "6 17 0.0021397105097545627\n",
      "7 2 0.0002517306482064191\n",
      "8 104 0.013089993706733794\n",
      "9 5 0.0006293266205160479\n",
      "10 155 0.019509125235997484\n",
      "11 17 0.0021397105097545627\n",
      "12 14 0.001762114537444934\n",
      "13 21 0.0026431718061674008\n",
      "14 5 0.0006293266205160479\n",
      "15 31 0.0039018250471994967\n",
      "16 3 0.0003775959723096287\n",
      "17 7330 0.9225928256765261\n",
      "18 12 0.0015103838892385148\n",
      "19 2 0.0002517306482064191\n"
     ]
    }
   ],
   "source": [
    "analyze_clusters(clusters_map, CLUSTERS_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно попытаться использовать какую-либо меру качества, не требующую наличия эталонной кластеризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def silhouette_score_for_samples(features, labels, metric='euclidean', sample_size=None, random_state=None, **kwds):\n",
    "    n_labels = len(np.unique(labels))\n",
    "    n_samples = features.shape[0]\n",
    "    if not 1 < n_labels < n_samples:\n",
    "        raise ValueError(\"Number of labels is %d. Valid values are 2 \"\n",
    "                         \"to n_samples - 1 (inclusive)\" % n_labels)\n",
    "\n",
    "    if sample_size is not None:\n",
    "        random_state = check_random_state(random_state)\n",
    "        indices = random_state.permutation(features.shape[0])[:sample_size]\n",
    "        if metric == \"precomputed\":\n",
    "            features, labels = features[indices].T[indices].T, labels[indices]\n",
    "        else:\n",
    "            features, labels = features[indices], labels[indices]\n",
    "    return silhouette_samples(features, labels, metric=metric, **kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples_scores = silhouette_score_for_samples(np.array(texts), np.array(clusters_map), metric='cosine')\n",
    "print(str(np.mean(samples_scores)) + \" \" + str(np.median(samples_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def bar_chart_for_samples_scores(samples_scores):\n",
    "    plt.clf()\n",
    "    plt.title(\"Distribution of silhouette scores for samples\")\n",
    "    plt.xlabel(\"Silhouette scores \")\n",
    "    plt.ylabel(\"Samples number\")\n",
    "    plt.axis([-1, 1, 0, len(samples_scores)])\n",
    "\n",
    "    plt.hist(samples_scores, bins=50)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "bar_chart_for_samples_scores(samples_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для случайной выборки текстов из корпуса каждый асессор может определять долю текстов, не соответствующих автоматически составленному описанию кластера в виде ключевых слов с точки зрения того, как он склонен ее интерпретировать. Итоговое значение ошибки необходимо подсчитывать для группы таких асессоров."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
